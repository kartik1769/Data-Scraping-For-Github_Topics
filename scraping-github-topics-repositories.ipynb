{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Top Repositories for Topics on Github\n",
    "\n",
    "TODO:\n",
    "- Web Scraping is a method where you extract large amount of data from a webpage/websites and use the extracted data for further data analysis.\n",
    "- In this project i will be scraping Github website which is a platform for people to work together on a project from anywhere. I will be using this website to scrape a particular domain of topics,these topics will be scraped according to their rating and popularity.\n",
    "- Tools used in this project are Python, requests, Beautiful Soup, Pandas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the steps we'll follow :\n",
    "- We're going to scrape https://github.com/topics\n",
    "- We'll get a list of Modules. For each module, we'll get module page URL and Module description\n",
    "- For each Module, we'll get the top 25 repositories in the module from the module page\n",
    "- For each repository, we'll grab the repo name , username ,rating & repo URL\n",
    "- For each module we'll create a CSV file in thr following format:\n",
    "``` \n",
    "username,repo_name,rating,repo_url\n",
    "ljianshu,Blog,7600,https://github.com/ljianshu/Blog\n",
    "metafizzy,infinite-scroll,7300,https://github.com/metafizzy/infinite-scroll\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape the list of Modules from Github\n",
    "\n",
    "- Using requests to download the page.\n",
    "- Use of BS4 to parse and extract information\n",
    "- Convert to a Pandas  DataFrame \n",
    "- Let's write a function to download the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "def get_Module_page():\n",
    "    Modules_url = 'https://github.com/topics'\n",
    "    response = requests.get(Modules_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception ('Failed to load Page {}'.format(topic_url))\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = get_Module_page()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this doc you can get particular information about the data you are searching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some helper functions to parse information from the page\n",
    "\n",
    "To get topic titles, we can pick `p` tags with the `class` ...\n",
    "![](https://i.imgur.com/CI7gVXU.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Module_titles(doc):\n",
    "    selection_class = 'f3 lh-condensed mb-0 mt-1 Link--primary'\n",
    "    Module_title_tags = doc.find_all('p',{'class' : selection_class})\n",
    "    Module_titles = []\n",
    "    for tag in Module_title_tags:\n",
    "        Module_titles.append(tag.text)\n",
    "    return Module_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_Module_Titles` can be used to get list of the titles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = get_Module_titles(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3D', 'Ajax', 'Algorithm', 'Amp', 'Android']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly we have defined functions for descriptions and URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Module_descs(doc) :\n",
    "    desc_selector = 'f5 color-fg-muted mb-0 mt-1'\n",
    "    Module_desc_tags = doc.find_all('p', {'class': desc_selector})\n",
    "    Module_descs = []\n",
    "    for tag in Module_desc_tags:\n",
    "        Module_descs.append(tag.text.strip())\n",
    "    return Module_descs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Along with the Modules we will also scrape there descriptions.\n",
    "- To do this we will select a particular unique tag like an identifier to get all the descriptions without any unwanted data.\n",
    "- A certain aspect should be considered which is while selecting a tag or a class from the htrml source code you have to select the identifiers as unique as possible which may not take any unwanted data\n",
    "![](https://i.imgur.com/lduZeCY.png)\n",
    "\n",
    "\n",
    "\n",
    "- for example here as p tags were already used for the titles but class for that title was different so we chose different class for description in html source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Module_urls(doc):\n",
    "    Module_link_tags = doc.find_all('a',{'class' : 'no-underline flex-1 d-flex flex-column'})\n",
    "    Module_urls = []\n",
    "    base_url = 'https://github.com'\n",
    "    for tag in Module_link_tags:\n",
    "        Module_urls.append(base_url + tag['href'])\n",
    "    return Module_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO :\n",
    "- As explained earlier it is similar here, just a different unique `a` tag and its corresponding `class` is used to get the urls which is added in a list and appended with the base github url  \n",
    "\n",
    "![](https://i.imgur.com/vk9iHwa.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put it altogether into a single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_Modules():\n",
    "    Modules_url = 'https://github.com/topics'\n",
    "    response = requests.get(Modules_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception ('Failed to load Page {}'.format(topic_url))\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    Module_dict = {\n",
    "        'title': get_Module_titles(doc),\n",
    "        'descriptions' : get_Module_descs(doc),\n",
    "        'url' : get_Module_urls(doc)\n",
    "    }\n",
    "    return pd.DataFrame(Module_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Top 25 Repositories from a Topic Page\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Module_page(Module_url):\n",
    "    # Downlod the Page\n",
    "    response = requests.get(Module_url)\n",
    "    # check successful response\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to Load Page {}'.format(Module_url))\n",
    "    # Parse using BeautifulSoup\n",
    "    Module_doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return Module_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Here, we get the module url, we use that url to check its status response to validate its usability to load the page\n",
    "- Next, we use Beautiful Soup to parse the HTML document into text format for further implementation of Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = get_Module_page('https://github.com/topic/3d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_rate_count(stars_str):\n",
    "    stars_str = stars_str.strip()\n",
    "    if stars_str[-1] == 'k' :\n",
    "        return int(float(stars_str[:-1]) * 1000)\n",
    "    return int(stars_str)\n",
    "\n",
    "def get_repo_info(h3_tag, rate_tag):\n",
    "    # returns all the required info about a repository\n",
    "    a_tags = h3_tag.find_all('a')\n",
    "    username = a_tags[0].text.strip()\n",
    "    repo_name = a_tags[1].text.strip()\n",
    "    base_url = 'https://github.com'\n",
    "    repo_url = base_url + a_tags[1]['href']\n",
    "    rating = parse_rate_count(rate_tag.text.strip())\n",
    "    return username, repo_name, rating, repo_url\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO \n",
    "- Here we used h3 as an identifier which is used to get all the indformation about the particular repositories like its rating, username etc.\n",
    "![](https://i.imgur.com/xiEmRjT.png)\n",
    "\n",
    "- h3 tag contains both the username and the repository corresponding to the username hence we used h3 for scraping the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Module_repos(Module_doc):\n",
    "    # Get the h3 tags containing  repo title, repo URL and username\n",
    "    h3_selection_class = 'f3 color-fg-muted text-normal lh-condensed'\n",
    "    repo_tags = Module_tags = Module_doc.find_all('h3', {'class' : h3_selection_class})\n",
    "    #Get Rating Tags\n",
    "    rate_tags = Module_doc.find_all('span', {'class' : 'Counter js-social-count'})\n",
    "    \n",
    "    Module_repos_dict = {\n",
    "        'username' : [],\n",
    "        'repo_name' : [],\n",
    "        'rating' : [],\n",
    "        'repo_url' : []\n",
    "    }\n",
    "    # Get repo info\n",
    "    for i in range(len(repo_tags)):\n",
    "        repo_info = get_repo_info(repo_tags[i], rate_tags[i])\n",
    "        Module_repos_dict['username'].append(repo_info[0])\n",
    "        Module_repos_dict['repo_name'].append(repo_info[1])\n",
    "        Module_repos_dict['rating'].append(repo_info[2])\n",
    "        Module_repos_dict['repo_url'].append(repo_info[3])\n",
    "        \n",
    "    return pd.DataFrame(Module_repos_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After Scraping the data we create a CSV file where all the generated data will be stored in unstructured manner or tabular format\n",
    "\n",
    "![](https://i.imgur.com/PYp5i8Z.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_Module(Module_url, path) :\n",
    "    if os.path.exists(path):\n",
    "        print(\"The file {} already exists. Skipping...\".format(path))\n",
    "        return\n",
    "    Module_df = get_Module_repos(get_Module_page(Module_url))\n",
    "    Module_df.to_csv(path,index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here we are generating and saving the scraped data in a CSV file which will be stored in OS in a defined path.\n",
    "\n",
    "![](https://i.imgur.com/vts9rMa.png)\n",
    "\n",
    "\n",
    "- Here the index is set to none because we won't be requiring index while scraping the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it altogether\n",
    "- We have a function to get the list of topics\n",
    "- We have a function to create a CSV file for scraped repos from a topics page\n",
    "- Let's create a function to put them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_Module_repos():\n",
    "    print('Scraping list of Modules from Github')\n",
    "    Module_df = scrape_Modules()\n",
    "    \n",
    "    # Create folder here\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    \n",
    "    for index, row in Module_df.iterrows():\n",
    "        print('Scraping Top Repositories for \"{}\"'.format(row['title']))\n",
    "        scrape_Module(row['url'], 'data/{}.csv'.format(row['title']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run it to scrape the top repositories for all the topics on the first page of https://github.com/topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping list of Modules from Github\n",
      "Scraping Top Repositories for \"3D\"\n",
      "Scraping Top Repositories for \"Ajax\"\n",
      "Scraping Top Repositories for \"Algorithm\"\n",
      "Scraping Top Repositories for \"Amp\"\n",
      "Scraping Top Repositories for \"Android\"\n",
      "Scraping Top Repositories for \"Angular\"\n",
      "Scraping Top Repositories for \"Ansible\"\n",
      "Scraping Top Repositories for \"API\"\n",
      "Scraping Top Repositories for \"Arduino\"\n",
      "Scraping Top Repositories for \"ASP.NET\"\n",
      "Scraping Top Repositories for \"Atom\"\n",
      "Scraping Top Repositories for \"Awesome Lists\"\n",
      "Scraping Top Repositories for \"Amazon Web Services\"\n",
      "Scraping Top Repositories for \"Azure\"\n",
      "Scraping Top Repositories for \"Babel\"\n",
      "Scraping Top Repositories for \"Bash\"\n",
      "Scraping Top Repositories for \"Bitcoin\"\n",
      "Scraping Top Repositories for \"Bootstrap\"\n",
      "Scraping Top Repositories for \"Bot\"\n",
      "Scraping Top Repositories for \"C\"\n",
      "Scraping Top Repositories for \"Chrome\"\n",
      "Scraping Top Repositories for \"Chrome extension\"\n",
      "Scraping Top Repositories for \"Command line interface\"\n",
      "Scraping Top Repositories for \"Clojure\"\n",
      "Scraping Top Repositories for \"Code quality\"\n",
      "Scraping Top Repositories for \"Code review\"\n",
      "Scraping Top Repositories for \"Compiler\"\n",
      "Scraping Top Repositories for \"Continuous integration\"\n",
      "Scraping Top Repositories for \"COVID-19\"\n",
      "Scraping Top Repositories for \"C++\"\n"
     ]
    }
   ],
   "source": [
    "scrape_Module_repos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that the CSVs were created properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}